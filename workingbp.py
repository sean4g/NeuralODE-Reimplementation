# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G6MV1F7McCIorA1boHgTl_95nCcX10Rc
"""

import time
from typing import List, Dict
import scipy.integrate
from scipy.integrate import solve_ivp
import autograd.numpy as np
import torch
from torch import Tensor
import torchvision.models as models
import torchvision.transforms as transforms
import torch.nn.functional as F
import torchvision.datasets
import torch.nn as nn
import torch.optim as optim
import autograd.numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

a = 34869261 #m
mu = 3.9860064E+14 #ùëö3/ùë†2
R = 6378139 #m

# Orbit period is calculated through Kepler's Third Law:
T = np.sqrt(a**3*(4*np.pi**2/mu)) #s
time_step = T/400

import torch
import numpy as np

def ABM(model, x0, sf, s0,aug=0):
    # Constants
    a = 34869261  # m
    mu = 3.9860064E+14  # m^3/s^2

    # Orbit period calculated through Kepler's Third Law
    T = np.sqrt(a**3 * (4 * np.pi**2 / mu))
    ########## FIXXXX
    ds = T/400
    #print("Step size (ds):", ds)

    # Calculate number of steps based on the given time interval and step size
    s = np.arange(s0, sf, ds)
    ns = len(s)  # Get the exact number of elements in s

    # Ensure x0 is a torch tensor with the correct shape
    if not isinstance(x0, torch.Tensor):
        x0 = torch.tensor(x0, dtype=torch.float32)
    if x0.dim() == 1:
        x0 = x0.unsqueeze(0)  # Ensures x0 is [1, 6] if it's provided as [6]

    # Initialize the tensor to store the simulation results
    x = torch.zeros((ns, x0.size(1)), dtype=x0.dtype, device=x0.device)
    # print("x0",x0)
    # Set initial state
    x[0, :] = x0.squeeze()  # Make sure x0 is squeezed to [6]
    f = model.func
    theta = model.theta
    # First initialize with an RK4 step for stability in starting the integration
    for k in range(3):
        if k + 1 < ns:
            k1 = ds * f(s[k], x[k, :],theta,aug)
            k2 = ds * f(s[k] + ds/2, x[k, :] + k1/2,theta,aug)
            k3 = ds * f(s[k] + ds/2, x[k, :] + k2/2,theta,aug)
            k4 = ds * f(s[k] + ds, x[k, :] + k3,theta,aug)
            dx = (k1 + 2*k2 + 2*k3 + k4) / 6
            x[k + 1, :] = x[k, :] + dx

    # ABM integration
    for k in range(3, ns - 1):
        if k - 3 >= 0:  # Make sure indices don't go out of bounds
            f_m3 = f(s[k-3], x[k-3, :],theta,aug)
            f_m2 = f(s[k-2], x[k-2, :],theta,aug)
            f_m1 = f(s[k-1], x[k-1, :],theta,aug)
            f_0 = f(s[k], x[k, :],theta,aug)

            # Predictor
            dx = (ds/24) * (55 * f_0 - 59 * f_m1 + 37 * f_m2 - 9 * f_m3)
            x[k + 1, :] = x[k, :] + dx

            # Evaluate at the predicted next step (ensure not at the last step)
            if k + 1 < ns - 1:
                f_p1 = f(s[k + 1], x[k + 1, :],theta,aug)
                # Corrector
                dx = (ds/24) * (9 * f_p1 + 19 * f_0 - 5 * f_m1 + f_m2)
                x[k + 1, :] = x[k, :] + dx

    # Return the results

    return x

def ABM_aug(f, x0, sf, s0,theta,aug=1):
    # Constants
    a = 34869261  # m
    mu = 3.9860064E+14  # m^3/s^2
    aug = 1
    # Orbit period calculated through Kepler's Third Law
    T = np.sqrt(a**3 * (4 * np.pi**2 / mu))
    ########## FIXXXX
    ds = T/400
    #print("Step size (ds):", ds)

    # Calculate number of steps based on the given time interval and step size
    s = np.arange(s0, sf, ds)
    ns = len(s)  # Get the exact number of elements in s

    # Ensure x0 is a torch tensor with the correct shape
    if not isinstance(x0, torch.Tensor):
        x0 = torch.tensor(x0, dtype=torch.float32)
    if x0.dim() == 1:
        x0 = x0.unsqueeze(0)  # Ensures x0 is [1, 6] if it's provided as [6]

    # Initialize the tensor to store the simulation results
    x = torch.zeros((ns, x0.size(1)), dtype=x0.dtype, device=x0.device)
    # print("x0",x0)
    # Set initial state
    x[0, :] = x0.squeeze()  # Make sure x0 is squeezed to [6]
    # First initialize with an RK4 step for stability in starting the integration
    for k in range(3):
        if k + 1 < ns:
            k1 = ds * f(s[k], x[k, :],theta,aug)
            k2 = ds * f(s[k] + ds/2, x[k, :] + k1/2,theta,aug)
            k3 = ds * f(s[k] + ds/2, x[k, :] + k2/2,theta,aug)
            k4 = ds * f(s[k] + ds, x[k, :] + k3,theta,aug)
            dx = (k1 + 2*k2 + 2*k3 + k4) / 6
            x[k + 1, :] = x[k, :] + dx

    # ABM integration
    for k in range(3, ns - 1):
        if k - 3 >= 0:  # Make sure indices don't go out of bounds
            f_m3 = f(s[k-3], x[k-3, :],theta,aug)
            f_m2 = f(s[k-2], x[k-2, :],theta,aug)
            f_m1 = f(s[k-1], x[k-1, :],theta,aug)
            f_0 = f(s[k], x[k, :],theta,aug)

            # Predictor
            dx = (ds/24) * (55 * f_0 - 59 * f_m1 + 37 * f_m2 - 9 * f_m3)
            x[k + 1, :] = x[k, :] + dx

            # Evaluate at the predicted next step (ensure not at the last step)
            if k + 1 < ns - 1:
                f_p1 = f(s[k + 1], x[k + 1, :],theta,aug)
                # Corrector
                dx = (ds/24) * (9 * f_p1 + 19 * f_0 - 5 * f_m1 + f_m2)
                x[k + 1, :] = x[k, :] + dx

    # Return the results

    return x

def X_ddot(t, X,theta,aug):
    mu = 3.9860064E+14
    '''
    Returns the derivative of the state vector X
    ----------
    Arguments:
        t {float} -- time, in seconds
        X {np.array} -- state vector=(x, y, z, vx, vy, vz)
    ----------
    Returns:
        (6,1) np.array -- (xdot, ydot, zdot, vxdot, vydot, vzdot)
    '''
    x_dot = X[3:]
    v_dot = -mu*X[:3]/np.linalg.norm(X.detach().numpy())**3 # simply the acceleration

    X_dot_dot = np.concatenate((x_dot.detach().numpy(), v_dot.detach().numpy()), axis=None)

    X_dot_dot = torch.tensor(X_dot_dot, dtype=torch.float32, requires_grad=True)
    if not aug:
      X_dot_dot = theta*X_dot_dot
    return X_dot_dot

def X_ddottrue(t, X,theta,aug):
    mu = 3.9860064E+14
    '''
    Returns the derivative of the state vector X
    ----------
    Arguments:
        t {float} -- time, in seconds
        X {np.array} -- state vector=(x, y, z, vx, vy, vz)
    ----------
    Returns:
        (6,1) np.array -- (xdot, ydot, zdot, vxdot, vydot, vzdot)
    '''
    x_dot = X[3:]
    v_dot = -mu*X[:3]/np.linalg.norm(X.detach().numpy())**3 # simply the acceleration

    X_dot_dot = np.concatenate((x_dot.detach().numpy(), v_dot.detach().numpy()), axis=None)
    X_dot_dot = torch.tensor(X_dot_dot, dtype=torch.float32, requires_grad=True)
    return X_dot_dot

# we have to create a custom forward and backwards pass
# use torch.autograd.Function for this purpose
class ODEForwardBackward(torch.autograd.Function):
    @staticmethod
    def forward(ctx, z0, theta ,t0 , t1,model):
        # Ensure initial state and time vector are properly formatted
        z0 = z0.squeeze()  # Correct shape if necessary
        z = torch.zeros((z0.size(0)), dtype=z0.dtype, device=z0.device)

        # Simulate the dynamics over the given time frame
        z = ABM(model, z0, t1, t0,aug=0)
        # print("forward z",z[-1])
        # Save for backward pass
        ctx.save_for_backward(z[-1], t0,t1, theta)
        ctx.model = model

        # Return the final state
        return z[-1]


    @staticmethod
    # pytorch AUTOMATICALLY gives us the loss gradient over the entire function.
    # therefore, from that gradient, we must return dldz0, dldt, and dldp
    # this is outlined in the appendix
    def backward(ctx, grad_zt1):
        z,t0,t1,theta = ctx.saved_tensors
        zt1 = z
        #print("BACK")
        #print("zt1",zt1)
        #print("t0",t0)
        #print("t1",t1)
        model = ctx.model
        #print("num parma",len(theta))
        #print("theta",theta)

        # Reconstruct augmented state at final time
        grad_t1 = grad_zt1 * model.func(t1,zt1,theta,aug=0)[-1]  # Assuming scalar t

        # print("BACK2")
        # print(grad_t1)
        aug_state0 = torch.cat([zt1, -grad_zt1, torch.zeros_like(theta).flatten(), -grad_t1])
        # Reverse time ODE solve (backpropagate from t1 to t0) THIS IS WRONG
        #audited_dynamics = Augmented_dynamics(model=model)
        #print("aug_state0",aug_state0)
        aug_state_t0 = ABM_aug(model.func,aug_state0, t1, t0,model.adtheta,0)

        # Extract gradients w.r.t. initial state, parameters, and initial and final times
        #print("aug_0",aug_state_t0)
        # print("HEEE",aug_state_t0.size)
        #print("aug_0",aug_state_t0[-1])
        aug_state_t0 = aug_state_t0[-1]

        #print("z0_aug",aug_state_t0[0:6] )
        #print("grad_z0",aug_state_t0[6:12])
        grad_z0 = aug_state_t0[6:12].unsqueeze(0)
        #print("grad_theta",aug_state_t0[12:12+len(theta)])
        grad_theta = aug_state_t0[12:12+len(theta)].unsqueeze(0)
        #print("grad_t0/grad_t1",aug_state_t0[12+len(theta):12+len(theta)+6],grad_t1)
        grad_t0 = aug_state_t0[12+len(theta):].unsqueeze(0)

        # Return gradients
        return grad_z0, grad_theta, grad_t0, grad_t1,None

class NeuralODE(nn.Module):
  def __init__(self,func,time_scale):
    super().__init__()
    self.func = func
    self.time_scale = time_scale
    self.adtheta = nn.Parameter(torch.ones(6*4, requires_grad=True))
    self.theta = nn.Parameter(torch.ones(6, requires_grad=True))

  def forward(self, z0, t0,t1):
    # Pass the input through the function
    return ODEForwardBackward.apply(z0, self.theta, t0, t1, self)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import torch
from torch.utils.data import Dataset, DataLoader

data = pd.read_csv('jan_train.csv')

# change this as needed for the number of satellites used
data = data[data['sat_id'].isin([0, 1, 2, 3, 4, 5, 6])]

# Convert time to seconds since first day
data['epoch'] = pd.to_datetime(data['epoch'])
reference_time = pd.Timestamp('2014-01-01 00:00:00.000')
data['time'] = (data['epoch'] - reference_time).dt.total_seconds()
data.drop(columns=['epoch'], inplace=True)
data['time'] = data['time'].astype(int)

# Fill in NA cells with average
data.fillna(data.mean(), inplace=True)

# Remove outliers
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_mask = ((data < lower_bound) | (data > upper_bound)).any(axis=1)
data = data[~outlier_mask]

# Normalize between -1 and 1
numeric_cols = data.select_dtypes(include=[np.number]).columns.difference(['time', 'id', 'sat_id'])
scaler = MinMaxScaler(feature_range=(-1, 1))
data.loc[:, numeric_cols] = scaler.fit_transform(data[numeric_cols])

data[:10]

#get the processed data into dataloader, ready for use in the network
class SatelliteDataset(Dataset):
    def __init__(self, features, targets):
        self.features = features
        self.targets = targets

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

features = data[['x', 'y', 'z', 'Vx', 'Vy', 'Vz']].values
targets = data[['x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim']].values

time = data['time'].values
time = time.reshape(-1, 1)
time_tensor = torch.tensor(time, dtype=torch.float32)
# convert to tensors
features_tensor = torch.tensor(features, dtype=torch.float32)
targets_tensor = torch.tensor(targets, dtype=torch.float32)

dataset = SatelliteDataset(features_tensor, targets_tensor)

dataloader = DataLoader(dataset[:,:740], batch_size=1, shuffle=False)

# model evaluation

model = NeuralODE(X_ddot,5)

# loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# training


model.train()
epochs = 5
# for now, do batch size of 1, for simpler adjustments in ODE
for epoch in range(epochs):
  total_loss = 0
  j = 0
  for x_1, y_1 in dataloader:
    x_1 = torch.autograd.Variable(x_1, requires_grad=True)
    y_1 = torch.autograd.Variable(y_1, requires_grad=True)

    y_pred = model(x_1, time_tensor[0] , time_tensor[j+1])

    optimizer.zero_grad()
    loss = criterion(y_pred.unsqueeze(0), y_1)
    total_loss += loss.item()
    loss.backward()
    optimizer.step()
    j+=1
  average_loss = total_loss / j-1
  print ("Epoch:", epoch, "Average Loss:", average_loss)


{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List, Dict\n",
        "\n",
        "import autograd.numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "0W3OiunZIAor"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test functions for our network / resnet18"
      ],
      "metadata": {
        "id": "vpvwbiOPZ0tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, loss_fn: nn.modules.loss._Loss, optimizer: torch.optim.Optimizer, train_loader: torch.utils.data.DataLoader, epoch: int=0)-> List:\n",
        "\n",
        "    train_loss = []\n",
        "    model.train()\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader ):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        #model.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimiz\n",
        "        t = torch.linspace(0, 64, steps=64)\n",
        "        outputs = model(images,t)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss)\n",
        "        if batch_idx % int(len(train_loader.dataset)/(10*len(images))) == 0 :\n",
        "          print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item():.3f}')\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "def test(model: nn.Module, loss_fn: nn.modules.loss._Loss, test_loader: torch.utils.data.DataLoader,epoch: int=0)-> Dict:\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  predictions = torch.Tensor()\n",
        "  predictions = predictions.to(device)\n",
        "  with torch.no_grad():\n",
        "    for images, targets in test_loader:\n",
        "      images = images.to(device)\n",
        "      targets = targets.to(device)\n",
        "      output = model(images)\n",
        "      test_loss += loss_fn(output, targets).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      predictions = torch.cat((predictions,pred),0)\n",
        "      correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    total_num = len(test_loader.dataset)\n",
        "    test_stat = {\n",
        "      \"loss\": test_loss,\n",
        "      \"accuracy\": accuracy,\n",
        "      \"prediction\": predictions,\n",
        "    }\n",
        "    print(f\"Test result on epoch {epoch}: total sample: {total_num}, Avg loss: {test_stat['loss']:.3f}, Acc: {100*test_stat['accuracy']:.3f}%\")\n",
        "    return test_stat"
      ],
      "metadata": {
        "id": "FCg0pz_QHPgl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader for Fashion mnist"
      ],
      "metadata": {
        "id": "znSwz3skZ6LD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SN3b-1j2HHTJ"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the dataset\n",
        "Batch_size = 64\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "trainset = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=Batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defeintions of classes and methods for Resdiaul ODE netowrk with 1 ode layer\n"
      ],
      "metadata": {
        "id": "OQJmnpnfZ9cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_f(z, t, p, a, module):\n",
        "  '''\n",
        "  Calculates the gradient of the function f with respect to z, t and p (parameters theta).\n",
        "\n",
        "  ---\n",
        "  Inputs:\n",
        "    z: np.array\n",
        "      Hidden state\n",
        "    t: float\n",
        "      Time\n",
        "    p: ##type##\n",
        "      Parameters, theta\n",
        "    a: ##type##\n",
        "      Adjoint, a\n",
        "    module: function\n",
        "      Function f\n",
        "  ---\n",
        "  Outputs:\n",
        "    adfdz: ##type##\n",
        "      Derivative of f wrt z\n",
        "    adfdt: ##type##\n",
        "      Derivative of f wrt t\n",
        "    adfdp: ##type##\n",
        "      Derivative of f wrt theta\n",
        "    f: ##type##\n",
        "      Function f evaluated at given z and t\n",
        "  '''\n",
        "  with torch.set_grad_enabled(True):\n",
        "    # ensure that we can find gradients using autograd\n",
        "    z.requires_grad_(True)\n",
        "    t.requires_grad_(True)\n",
        "    p.requires_grad_(True)\n",
        "\n",
        "    # calculate output f; inputs are z, t\n",
        "    f = module(z, t)\n",
        "    # p = module.parameters() # in case we need the parameters after function calculation ?\n",
        "    # torch autograd grad with grad_outputs computes jacobian product\n",
        "    adfdz = torch.autograd.grad(f, z, grad_outputs=(a), allow_unused=True)\n",
        "    adfdt = torch.autograd.grad(f, t, grad_outputs=(a), allow_unused=True)\n",
        "    adfdp = torch.autograd.grad(f, p, grad_outputs=(a), allow_unused=True)\n",
        "\n",
        "  return adfdz, adfdt, adfdp, f\n",
        "\n",
        "def aug_dynamics(z, t, theta, a, module):\n",
        "  '''\n",
        "  Defines dynamics of augmented state.\n",
        "  ---\n",
        "  Inputs:\n",
        "    z: np.array\n",
        "      Hidden state\n",
        "    t: float\n",
        "      Time\n",
        "    theta: ##type##\n",
        "      Dynamic parameters\n",
        "    a: ##type##\n",
        "      Adjoint, a\n",
        "    delL_deltheta: ##type##\n",
        "      Derivative of loss wrt theta\n",
        "    delL_delt: ##type##\n",
        "      Derivative of loss wrt t\n",
        "    module: function\n",
        "      Function f\n",
        "\n",
        "  ---\n",
        "\n",
        "  Returns:\n",
        "    delz_delt: np.array\n",
        "      Time derivative of state, z\n",
        "\n",
        "    dela_delt: ##[type]##\n",
        "      Time derivative of adjoint, a\n",
        "\n",
        "    deldelL_deltheta_delt: ##[type]##\n",
        "      Time derivative of loss gradient wrt dynamic parameters, delL_deltheta\n",
        "\n",
        "  '''\n",
        "  with torch.enable_grad():\n",
        "      z = z.detach().requires_grad_(True)\n",
        "      f = module(z, t, theta)  # Evaluate the ODE function at z, t, and theta\n",
        "\n",
        "      # Compute gradients of f with respect to z and theta\n",
        "      df_dz, df_dtheta = torch.autograd.grad(f, (z, theta), grad_outputs=a, create_graph=True)\n",
        "\n",
        "      # Time derivative of state, z\n",
        "      delz_delt = df_dz\n",
        "\n",
        "      # Time derivative of adjoint, a\n",
        "      dela_delt = -a.T @ df_dz  # Vector-Jacobian product\n",
        "\n",
        "      # Time derivative of loss gradient wrt dynamic parameters, delL_deltheta\n",
        "      deldelL_deltheta_delt = -a.T @ df_dtheta  # Vector-Jacobian product\n",
        "  return delz_delt, dela_delt, deldelL_deltheta_delt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ode_solve_augstate(s1, t0, t1, dynamics, z, theta, module):\n",
        "  '''Solves the ODE for the augmented state dynamics.'''\n",
        "\n",
        "  # Initializing parameters\n",
        "  h = .5 # step size\n",
        "  t = np.linspace(t0, t1, int((abs(t1 - t0))/h))\n",
        "\n",
        "  # Initial variables at t1\n",
        "  s = s1 # start with s1\n",
        "  a = s[1] #adjoint is the second element of s\n",
        "\n",
        "\n",
        "  # Integrating for each timestep\n",
        "  for t in range(len(t)):\n",
        "\n",
        "    # Euler method\n",
        "    s = s - h * dynamics(z, t, theta, a, module)\n",
        "\n",
        "    #Update z and a\n",
        "    z = s[0] #hidden state is the first element of s\n",
        "    a = s[1] #adjoint is the second element of s\n",
        "\n",
        "  return s1\n",
        "\n",
        "\n",
        "\n",
        "# we have to create a custom forward and backwards pass\n",
        "def gradient_loss(theta, t0, t1, zt1, delL_delzt1, module):\n",
        "  '''\n",
        "  Reverse-mode derivative of an ODE initial value problem.\n",
        "  Returns gradients of the loss.\n",
        "  ---\n",
        "  Inputs:\n",
        "    theta: np.array\n",
        "      Dynamic parameters\n",
        "\n",
        "    t0: float\n",
        "      Start time\n",
        "\n",
        "    t1: float\n",
        "      Stop time\n",
        "\n",
        "    zt0:\n",
        "      initial state\n",
        "\n",
        "    zt1: np.array\n",
        "      Final state\n",
        "\n",
        "    delL_delzt1: ##[type]##\n",
        "      Loss gradient at stop time\n",
        "\n",
        "  ---\n",
        "\n",
        "  Returns:\n",
        "    delL_delzt0: ##[type]##\n",
        "      Loss gradient at start time\n",
        "\n",
        "    delL_deltheta: ##[type]##\n",
        "      Loss gradient wrt dynamic parameters\n",
        "\n",
        "    delL_delt0: ##[type]##\n",
        "      Loss gradient wrt initial time\n",
        "\n",
        "    delL_delt1: ##[type]##\n",
        "      Loss gradient wrt stop time\n",
        "\n",
        "  '''\n",
        "  print(\"test b.1\")\n",
        "  # Calculate f(z(t1), t1, theta)\n",
        "  ft1 = module(zt1, t1, theta)\n",
        "\n",
        "  # Calculate gradient of loss wrt t1\n",
        "  delL_delt1 = delL_delzt1.T@ft1\n",
        "\n",
        "  # Define initial augmented state\n",
        "  s1 = np.array([zt1, delL_delzt1, torch.zeros(theta.shape[0]), -delL_delt1]) #s1 = [zt1, delL_delzt1, delL_deltheta1, -delL_delt1]\n",
        "\n",
        "  # Solve reverse-time ODE\n",
        "  s0 = ode_solve_augstate(s1, t0, t1, aug_dynamics, zt1, theta, module)\n",
        "  s0 = s0\n",
        "  print(\"test b.2\")\n",
        "  #s0 = [zt0, delL_delzt0, delL_deltheta0, -delL_delt0]\n",
        "  zt0 = s0[0]\n",
        "  delL_delzt0 = s0[1]\n",
        "  delL_deltheta = s0[2]\n",
        "  delL_delt0 = -s0[3]\n",
        "\n",
        "  # Return gradients\n",
        "  return delL_delzt0, delL_deltheta, delL_delt0, delL_delt1\n",
        "\n",
        "\n",
        "\n",
        "class ODEForwardBackward(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, p, func):\n",
        "      '''\n",
        "      Finds the z values by solving the ODE\n",
        "      ---\n",
        "      Inputs:\n",
        "        z0: initial state\n",
        "        t: time vector\n",
        "        p: parameters\n",
        "        func: f(z(t), t, theta)\n",
        "      ---\n",
        "      Returns:\n",
        "        z: filled z matrix\n",
        "      '''\n",
        "      # because we have our own backwards method, we do not need to track any gradients:\n",
        "      # for now lets assume z is a vector ( tesnor ), no batch\n",
        "      with torch.enable_grad():\n",
        "        t_len = t.size(0)\n",
        "        new_shape = (t_len, *z0.size())\n",
        "        z = torch.zeros([t_len] + list(z0.size()), device=z0.device)\n",
        "        z[0] = z0\n",
        "        for i in range(t_len -1):\n",
        "          # func is a nn.Module. it should apply forward when calling func(--)\n",
        "          res = ode_solve(z0, t[i], t[i+1], func)\n",
        "          z[i] = res\n",
        "      # save the function and t, z, p for backwards pass\n",
        "      ctx.func = func\n",
        "      ctx.save_for_backward(z,t ) # !!! save the paramteters in time step you want !!!!\n",
        "      ctx.params = p\n",
        "\n",
        "      return  z[1]\n",
        "\n",
        "    @staticmethod\n",
        "    # pytorch AUTOMATICALLY gives us the loss gradient over the entire function.\n",
        "    # therefore, from that gradient, we must return dldz0, dldt, and dldp\n",
        "    # this is outlined in the appendix\n",
        "    def backward(ctx, loss_grad):\n",
        "      '''\n",
        "      Finds our gradients wrt our inputs in forward pass\n",
        "      ---\n",
        "      Inputs:\n",
        "        loss_grad: total loss gradient over z\n",
        "      ---\n",
        "      Returns:\n",
        "        grad_z0: Loss gradient wrt z0\n",
        "        grad_t: loss gradient wrt t\n",
        "        grad_p: loss gradient wrt parameters\n",
        "      '''\n",
        "\n",
        "      #adjoint is dldz\n",
        "      a = loss_grad\n",
        "      # how can we get the gradients wrt our inputs?\n",
        "      # dAaug/dt = -[a∂f/∂z, a∂f/∂θ, a∂f/∂t], integrate both sides\n",
        "      with torch.enable_grad():\n",
        "      # get our saved tensors\n",
        "        func = ctx.func\n",
        "        z, t, p = ctx.saved_tensors\n",
        "        adfdz, adfdt, adfdp, out = grad_f(z, t, p, a, func)\n",
        "\n",
        "        if adfdz[0] is None:\n",
        "          adfdz = torch.zeros(z.size())\n",
        "        if adfdt[0] is None:\n",
        "          adfdt = torch.zeros(t.size())\n",
        "        if adfdp[0] is None:\n",
        "          adfdp = torch.zeros(p.size())\n",
        "\n",
        "      # we can now solve for our gradients by using augmented dynamics\n",
        "      # can we do it one by one too?\n",
        "        for i in range(len(t) - 1, 1, -1):\n",
        "          grad_z0 = adjoint_solve(a, t[i], t[i-1], -adfdz[-1])\n",
        "          grad_t = adjoint_solve(t, t[i], t[i-1], -adfdt[-1])\n",
        "          grad_p = adjoint_solve(p, t[i], t[i-1], -adfdp[-1])\n",
        "\n",
        "      return grad_z0, grad_t, grad_p\n",
        "\n",
        "\n",
        "#basic ODE solver ( maybe replace with RPK45 ) curretnly eurler method\n",
        "def ode_solve(z0, t0, t1, f):\n",
        "  h = .5 # step size\n",
        "  t = np.linspace(t0, t1, int((abs(t1 - t0))/h))\n",
        "  z = z0\n",
        "  for i in range(len(t)-1):\n",
        "    z = z + h * f(z, t[i])\n",
        "  return z\n",
        "\n",
        "def adjoint_solve(atf, t0, t1, grad_f):\n",
        "  # solve the ode backwards\n",
        "  h = 0.05\n",
        "  steps = int(abs(t1 - t0) / h)\n",
        "  t = np.linspace(t0, t1, steps + 1)\n",
        "  z = atf\n",
        "  for i in range(1, len(t)):\n",
        "    time = t[i]\n",
        "    z = z + h * grad_f\n",
        "  return z\n",
        "\n",
        "\n",
        "class ResidualBlock_ODE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x, theta):\n",
        "        if x.shape[0] == 2:\n",
        "          # Sometimes the extra time channel makes it into this step which is a no no\n",
        "          x = x[1]\n",
        "        #basic reisdaul layer to be ODE magicify\n",
        "        out = self.norm(F.relu(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "class ResODE(nn.Module):\n",
        "    def __init__(self, ode):\n",
        "        super().__init__()\n",
        "        self.ode = ode\n",
        "\n",
        "    def forward(self, z0, t=torch.tensor([0., 1.])):\n",
        "        t = t.to(z0)\n",
        "\n",
        "        # the magic awful break\n",
        "        z = ODEForwardBackward.apply(z0, t, self.parameters , self.ode)\n",
        "        return z\n",
        "\n",
        "class ODE_model(nn.Module):\n",
        "    def __init__(self, ode_step):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ODESTEP = ode_step\n",
        "        self.fc = nn.Linear(64*7 * 7, 10)\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x,t):\n",
        "        # 2 down sampling layers\n",
        "        x = self.pool(F.relu(self.batchnorm(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.batchnorm(self.conv2(x))))\n",
        "        # Resdiaul ODE layer\n",
        "        x = self.ODESTEP(x)\n",
        "        # fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rF_yRNIPH7UK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "max_epoch = 3\n",
        "\n",
        "\n",
        "#our model\n",
        "t = torch.tensor([0., 1.])\n",
        "\n",
        "Ode_model= ODE_model(ResODE(ResidualBlock_ODE()))\n",
        "Ode_model = Ode_model.to(device)\n",
        "#RESNET Time\n",
        "\n",
        "Res = models.resnet18()\n",
        "ODE_optim = torch.optim.SGD(Ode_model.parameters(), lr= 0.6, momentum=0.9)\n",
        "optimizerRES = optim.SGD(Res.parameters(), lr=0.01, momentum=0.8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "#train(Ode_net,criterion,optimizer,trainloader,epoch)\n",
        "\n",
        "Ode_model = Ode_model.to(device)\n",
        "for epoch in range(max_epoch):\n",
        "  train(Ode_model,criterion,ODE_optim,trainloader,epoch)\n",
        "test(Ode_model, criterion, testloader, epoch)\n",
        "\n",
        "# # Need to remove the 3 channel input (RGB) to 1 channel greyscale\n",
        "# Res.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "\n",
        "# start = time.time()\n",
        "\n",
        "# for epoch in range(max_epoch):\n",
        "#   train(Res,criterion,optimizerRES,trainloader,epoch)\n",
        "# test(Res, criterion, testloader, epoch)\n",
        "# end = time.time()\n",
        "# print(f'Finished Training after {end-start} s ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiZj45p1IWf4",
        "outputId": "a1bdf007-2565-4938-b614-6c18f4474e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: [0/60000] Loss: 2.304\n",
            "Epoch 0: [5952/60000] Loss: 2.303\n",
            "Epoch 0: [11904/60000] Loss: 2.303\n",
            "Epoch 0: [17856/60000] Loss: 2.302\n",
            "Epoch 0: [23808/60000] Loss: 2.304\n",
            "Epoch 0: [29760/60000] Loss: 2.304\n",
            "Epoch 0: [35712/60000] Loss: 2.301\n",
            "Epoch 0: [41664/60000] Loss: 2.303\n",
            "Epoch 0: [47616/60000] Loss: 2.303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUwbisCJlCDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
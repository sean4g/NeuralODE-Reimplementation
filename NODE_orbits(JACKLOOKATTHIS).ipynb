{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96aa9d-23a3-4be9-8ad8-4b3643899eb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b940d6-f4db-4391-b806-3643c733988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import scipy.integrate\n",
    "import autograd.numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f9ba0-b224-4854-990b-3cadbe852d21",
   "metadata": {},
   "source": [
    "# ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebee7f-e5df-412e-babf-414c7a6b4f24",
   "metadata": {},
   "source": [
    "This step was to solve the ODE. Here, an implemetation of the ABM method for integration was used to solve the differential equation in the form of $ \\frac{d\\vec{z}(t)}{dt} = f(\\vec{z}(t), t, \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3bc84-d0f5-49dc-bbd6-cf36cda746a3",
   "metadata": {},
   "source": [
    "## State vector derivative ($f = \\frac{dz}{dt}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9ca99-7912-4d42-a599-1c11d3ae5ef5",
   "metadata": {},
   "source": [
    "Here we define the derivative of the state vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2ca728-0632-4b65-967d-2e78e743ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_ddot(t, X):\n",
    "    '''\n",
    "    Returns the derivative of the state vector X\n",
    "    ----------\n",
    "    Arguments:\n",
    "        t {float} -- time, in seconds\n",
    "        X {np.array} -- state vector=(x, y, z, vx, vy, vz)\n",
    "    ----------\n",
    "    Returns:\n",
    "        (6,1) np.array -- (xdot, ydot, zdot, vxdot, vydot, vzdot)\n",
    "    '''\n",
    "    \n",
    "    x_dot = X[3:]\n",
    "    v_dot = -mu*X[:3]/np.linalg.norm(X.detach().numpy())**3 # simply the acceleration\n",
    "    X_dot_dot = np.concatenate((x_dot.detach().numpy(), v_dot.detach().numpy()), axis=None)\n",
    "\n",
    "    X_dot_dot = torch.tensor(X_dot_dot, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    return X_dot_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6278c9-a224-460c-9b9b-5b5b5efcbbce",
   "metadata": {},
   "source": [
    "## f derivative ($\\frac{df}{dt}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62db0b-0ae3-4a6e-a33b-a95e205b926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_dddot(t, X, mu):\n",
    "    '''\n",
    "    Calculates the second derivative of the gravitational velocity and the first derivative of the acceleration\n",
    "    ----------\n",
    "    Arguments:\n",
    "        t {float} -- time, in seconds\n",
    "        X {torch.tensor} -- state vector = (x, y, z, vx, vy, vz), requires_grad should be True\n",
    "        mu {float} -- standard gravitational parameter (GM)\n",
    "    ----------\n",
    "    Returns:\n",
    "        torch.tensor -- (vx_dot, vy_dot, vz_dot, ax_dot, ay_dot, az_dot)\n",
    "    '''\n",
    "\n",
    "    # Ensure X is a tensor with gradient computation enabled\n",
    "    if not X.requires_grad:\n",
    "        X = X.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Position (x, y, z) and velocity (vx, vy, vz)\n",
    "    position = X[:3]\n",
    "    velocity = X[3:]\n",
    "\n",
    "    # Compute gravitational acceleration\n",
    "    r = torch.norm(position)\n",
    "    acceleration = -mu * position / r**3\n",
    "\n",
    "    # Concatenate velocity and acceleration to form the state derivative\n",
    "    state_derivative = torch.cat((velocity, acceleration))\n",
    "\n",
    "    # Use automatic differentiation to compute the derivative of the state derivative\n",
    "    state_derivative_grad = torch.autograd.grad(outputs=state_derivative, inputs=X,\n",
    "                                                grad_outputs=torch.ones_like(state_derivative),\n",
    "                                                create_graph=True)[0]\n",
    "\n",
    "    return state_derivative_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace61dd9-5fa8-47e6-aefd-fd3f0ff1db66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ODE Solver (RK4 & ABM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f92f05-da6b-4fec-9492-692fcf1708ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RK4(f, x0, tf, dt, t0=0):\n",
    "    # Time vector\n",
    "    t = np.arange(t0, tf, dt)\n",
    "    nt = t.size\n",
    "    \n",
    "    # Constructing final vector\n",
    "    nx = x0.size\n",
    "    x = np.zeros((nx, nt))\n",
    "    \n",
    "    # Initial conditions\n",
    "    x[:,0] = x0\n",
    "    \n",
    "    for k in range(nt-1):\n",
    "        # At the kth time step, each parameter k_n is computed\n",
    "        k1 = dt*f(t[k], x[:, k])\n",
    "        k2 = dt*f(t[k] + dt/2, x[:, k] + k1/2)\n",
    "        k3 = dt*f(t[k] + dt/2, x[:, k] + k2/2)\n",
    "        k4 = dt*f(t[k] + dt, x[:, k] + k3)\n",
    "        \n",
    "        # k_ns are used to calculate dx\n",
    "        dx = (k1 + 2*k2 + 2* k3 + k4)/6\n",
    "        \n",
    "        #dx is used to calculate x at the next time step\n",
    "        x[:, k+1] = x[:, k] + dx\n",
    "        \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c46529-c7d7-4031-8888-2ff4349559fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABM_aug(f, x0, sf, s0,theta,aug=1):\n",
    "    # Constants\n",
    "    a = 600000  # m\n",
    "    mu = 3.9860064E+14  # m^3/s^2\n",
    "    aug = 0\n",
    "    # Orbit period calculated through Kepler's Third Law\n",
    "    T = np.sqrt(a**3 * (4 * np.pi**2 / mu))\n",
    "    ds = T/4000\n",
    "    #print(\"Step size (ds):\", ds)\n",
    "\n",
    "    # Calculate number of steps based on the given time interval and step size\n",
    "    s = np.arange(s0, sf, ds)\n",
    "    ns = len(s)  # Get the exact number of elements in s\n",
    "\n",
    "    # Ensure x0 is a torch tensor with the correct shape\n",
    "    if not isinstance(x0, torch.Tensor):\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32)\n",
    "    if x0.dim() == 1:\n",
    "        x0 = x0.unsqueeze(0)  # Ensures x0 is [1, 6] if it's provided as [6]\n",
    "\n",
    "    # Initialize the tensor to store the simulation results\n",
    "    x = torch.zeros((ns, x0.size(1)), dtype=x0.dtype, device=x0.device)\n",
    "    # print(\"x0\",x0)\n",
    "    # Set initial state\n",
    "    x[0, :] = x0.squeeze()  # Make sure x0 is squeezed to [6]\n",
    "    # First initialize with an RK4 step for stability in starting the integration\n",
    "    for k in range(3):\n",
    "        if k + 1 < ns:\n",
    "            k1 = ds * f(s[k], x[k, :],theta,aug)\n",
    "            k2 = ds * f(s[k] + ds/2, x[k, :] + k1/2,theta,aug)\n",
    "            k3 = ds * f(s[k] + ds/2, x[k, :] + k2/2,theta,aug)\n",
    "            k4 = ds * f(s[k] + ds, x[k, :] + k3,theta,aug)\n",
    "            dx = (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "            x[k + 1, :] = x[k, :] + dx\n",
    "\n",
    "    # ABM integration\n",
    "    for k in range(3, ns - 1):\n",
    "        if k - 3 >= 0:  # Make sure indices don't go out of bounds\n",
    "            f_m3 = f(s[k-3], x[k-3, :],theta,aug)\n",
    "            f_m2 = f(s[k-2], x[k-2, :],theta,aug)\n",
    "            f_m1 = f(s[k-1], x[k-1, :],theta,aug)\n",
    "            f_0 = f(s[k], x[k, :],theta,aug)\n",
    "\n",
    "            # Predictor\n",
    "            dx = (ds/24) * (55 * f_0 - 59 * f_m1 + 37 * f_m2 - 9 * f_m3)\n",
    "            x[k + 1, :] = x[k, :] + dx\n",
    "\n",
    "            # Evaluate at the predicted next step (ensure not at the last step)\n",
    "            if k + 1 < ns - 1:\n",
    "                f_p1 = f(s[k + 1], x[k + 1, :],theta,aug)\n",
    "                # Corrector\n",
    "                dx = (ds/24) * (9 * f_p1 + 19 * f_0 - 5 * f_m1 + f_m2)\n",
    "                x[k + 1, :] = x[k, :] + dx\n",
    "\n",
    "    # Return the results\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a27a3c-a6cd-47b6-ac2c-d79db25db298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABM(f, x0, sf, ds, s0=0):\n",
    "    # Time vector\n",
    "    s = np.arange(s0, sf, ds)\n",
    "    ns = s.size\n",
    "    \n",
    "    # Constructing final vector\n",
    "    nx = x0.size\n",
    "    x = np.zeros((nx, ns))\n",
    "    \n",
    "    # Initial conditions\n",
    "    x[:,0] = x0\n",
    "    \n",
    "    # First we initialize with an RK4:\n",
    "    for k in range(0, 3):\n",
    "        # At the kth time step, each parameter k_n is computed\n",
    "        k1 = ds*f(s[k], x[:, k])\n",
    "        k2 = ds*f(s[k] + ds/2, x[:, k] + k1/2)\n",
    "        k3 = ds*f(s[k] + ds/2, x[:, k] + k2/2)\n",
    "        k4 = ds*f(s[k] + ds, x[:, k] + k3)\n",
    "        \n",
    "        # k_ns are used to calculate dx\n",
    "        dx = (k1 + 2*k2 + 2* k3 + k4)/6\n",
    "        \n",
    "        #dx is used to calculate x at the next time step\n",
    "        x[:, k+1] = x[:, k] + dx\n",
    "    \n",
    "    # Proceeding to the ABM integration:\n",
    "    for k in range(3, ns-1):\n",
    "        f_m3 = f(s[k-3], x[:, k-3]) #f_{n-3}\n",
    "        f_m2 = f(s[k-2], x[:, k-2]) #f_{n-2}\n",
    "        f_m1 = f(s[k-1], x[:, k-1]) #f_{n-1}\n",
    "        f_0 = f(s[k], x[:, k]) #f_{n}\n",
    "        \n",
    "        ### Predictor ###\n",
    "        dx = (ds/24) * (55*f_0 - 59*f_m1 + 37*f_m2 - 9*f_m3)\n",
    "        x[:, k+1] = x[:, k] + dx\n",
    "        \n",
    "        f_p1 = f(s[k+1], x[:, k+1]) #f_{n+1}\n",
    "        \n",
    "        ### Corrector ###\n",
    "        dx = (ds/24) * (9*f_p1 + 19*f_0 - 5*f_m1 + f_m2)\n",
    "        #dx is used to calculate x at the next time step\n",
    "        x[:, k+1] = x[:, k] + dx\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd1a8a-affa-4958-984c-41749c6d53b3",
   "metadata": {},
   "source": [
    "# Augmented State Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac820ec5-a57b-46ea-b928-62ae203082d5",
   "metadata": {},
   "source": [
    "Next, we defined the dynamics of the augmented state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fd944-6088-423d-9975-77286b5ec792",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Augmented dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981032b0-9c18-4002-919f-f39a3b0c56fc",
   "metadata": {},
   "source": [
    "\n",
    "The next function, `aug_dynamics`, is used to calculate how the augmented state $\\vec{s}(t) = [\\vec{z}(t), \\vec{a}(t), \\frac{\\partial L}{\\partial\\theta}, \\frac{\\partial L}{\\partial t}]$ evolves over time. In other words, it returns $\\frac{d\\vec{s}(t)}{dt}$.\n",
    "\n",
    "This new version uses as inputs the gradient of the function f (parameterized by the NN described previously) with respect to z, t and $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46eb86a8-d600-4dc1-bf35-694826dc3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_dynamics_new(z, t, theta, a, module, df_dz, df_dt, df_dtheta):\n",
    "  '''\n",
    "  Defines dynamics of augmented state.\n",
    "  ---\n",
    "  Inputs:\n",
    "    z: np.array\n",
    "      Hidden state\n",
    "    t: float\n",
    "      Time\n",
    "    theta: ##type##\n",
    "      Dynamic parameters\n",
    "    a: ##type##\n",
    "      Adjoint, a\n",
    "    delL_deltheta: ##type##\n",
    "      Derivative of loss wrt theta\n",
    "    delL_delt: ##type##\n",
    "      Derivative of loss wrt t\n",
    "    module: function\n",
    "      Function f\n",
    "\n",
    "  ---\n",
    "\n",
    "  Returns:\n",
    "    delz_delt: np.array\n",
    "      Time derivative of state, z\n",
    "\n",
    "    dela_delt: ##[type]##\n",
    "      Time derivative of adjoint, a\n",
    "\n",
    "    deldelL_deltheta_delt: ##[type]##\n",
    "      Time derivative of loss gradient wrt dynamic parameters, delL_deltheta\n",
    "\n",
    "    delL_delt: ##[type]##\n",
    "      Time derivative of loss, L\n",
    "\n",
    "  '''\n",
    "  # Build augmented state: [z, a, delL_deltheta, delL_delt]\n",
    "  s = [z, a, _, _] # delL_deltheta and delL_delt are missing since we don't have them yet! But we won`t need them just yet\n",
    "\n",
    "  # Calculate f itself (delz_delt)\n",
    "  delz_delt = module(z, t)\n",
    "\n",
    "  # Time derivative of adjoint\n",
    "  dela_delt = -a.T@df_dz # Vector-jacobian product\n",
    "\n",
    "  # Time derivative of loss gradient\n",
    "  deldelL_deltheta_delt = -a.T@df_dtheta # Vector-jacobian product\n",
    "\n",
    "  # Time derivative of loss\n",
    "  delL_delt = -a.T@df_dt # Vector-jacobian product\n",
    "\n",
    "  return (delz_delt, dela_delt, deldelL_deltheta_delt, delL_delt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a79c329-b1d7-4274-be74-61b9ff70717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_dynamics(z, t, theta, a, module):\n",
    "  '''\n",
    "  Defines dynamics of augmented state.\n",
    "  ---\n",
    "  Inputs:\n",
    "    z: np.array\n",
    "      Hidden state\n",
    "    t: float\n",
    "      Time\n",
    "    theta: ##type##\n",
    "      Dynamic parameters\n",
    "    a: ##type##\n",
    "      Adjoint, a\n",
    "    delL_deltheta: ##type##\n",
    "      Derivative of loss wrt theta\n",
    "    delL_delt: ##type##\n",
    "      Derivative of loss wrt t\n",
    "    module: function\n",
    "      Function f\n",
    "\n",
    "  ---\n",
    "\n",
    "  Returns:\n",
    "    delz_delt: np.array\n",
    "      Time derivative of state, z\n",
    "\n",
    "    dela_delt: ##[type]##\n",
    "      Time derivative of adjoint, a\n",
    "\n",
    "    deldelL_deltheta_delt: ##[type]##\n",
    "      Time derivative of loss gradient wrt dynamic parameters, delL_deltheta\n",
    "\n",
    "    delL_delt: ##[type]##\n",
    "      Time derivative of loss, L\n",
    "\n",
    "  '''\n",
    "  # Build augmented state: [z, a, delL_deltheta, delL_delt]\n",
    "  s = [z, a, _, _] # delL_deltheta and delL_delt are missing since we don't have them yet! But we won`t need them just yet\n",
    "\n",
    "  # Calculate derivatives of f, as well as f itself (delz_delt)\n",
    "  delf_delz, delf_delt, delf_deltheta, delz_delt = grad_f(z, t, theta, a, module)\n",
    "\n",
    "  # Time derivative of adjoint\n",
    "  dela_delt = -a.T@delf_delz # Vector-jacobian product\n",
    "\n",
    "  # Time derivative of loss gradient\n",
    "  deldelL_deltheta_delt = -a.T@delf_deltheta # Vector-jacobian product\n",
    "\n",
    "  # Time derivative of loss\n",
    "  delL_delt = -a.T@delf_delt # Vector-jacobian product\n",
    "\n",
    "  return (delz_delt, dela_delt, deldelL_deltheta_delt, delL_delt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518c1cb-3311-4cff-b183-3d1cccf20773",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee3cac-6f92-4272-a6fd-9e1446175240",
   "metadata": {},
   "source": [
    "## Initial conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5dbc9-8662-4a93-9113-b82c4b8ec747",
   "metadata": {},
   "source": [
    "The initial conditions are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07ad3f2-9c2d-4cd7-aad7-a3cb785894a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1888980.04103698 #m\n",
    "y = 6652209.67475597 #m\n",
    "z = 902482.883545056 #m\n",
    "v_x = -9585.79511076297 #m/s\n",
    "v_y = 2413.57051166562 #m/s\n",
    "v_z = 2273.50409709003 #m/s\n",
    "\n",
    "x_vec = np.array([x,y,z])\n",
    "v_vec = np.array([v_x,v_y,v_z])\n",
    "\n",
    "a = 34869261 #m\n",
    "mu = 3.9860064E+14 #ð‘š3/ð‘ 2\n",
    "R = 6378139 #m\n",
    "\n",
    "# Orbit period is calculated through Kepler's Third Law:\n",
    "T = np.sqrt(a**3*(4*np.pi**2/mu)) #s\n",
    "delta_t = T/400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f08e7-3599-46b4-9731-2b653c82e30e",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ae86cf1-d0eb-4e86-adfd-6efba51d2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "model = nn.Linear(in_features=6, out_features=6)  # Change architecture as necessary\n",
    "\n",
    "# Example input and target\n",
    "state = torch.tensor(x0, dtype=torch.float32, requires_grad=True)\n",
    "#state = torch.randn(1, 6, requires_grad=True)  # State must require_grad to compute gradients w.r.t. it\n",
    "target = torch.randn(1, 6)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass\n",
    "prediction = model(state)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(prediction, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Gradient of the loss with respect to the state\n",
    "delL_delz1 = state.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0abb2f7b-07ea-41a7-8c42-0adb622cbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define (initial) state\n",
    "z = state\n",
    "\n",
    "# Define time tensor\n",
    "t = np.arange(t0, tf, dt)\n",
    "t = torch.tensor(t, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Define (initial) adjunct\n",
    "a = delL_delz1 # Gradient of the loss with respect to the state\n",
    "\n",
    "# Define theta collecting all parameters into a single vector\n",
    "theta_vector = torch.cat([p.view(-1) for p in model.parameters() if p.requires_grad])\n",
    "theta = theta_vector\n",
    "\n",
    "# Define function that goes inside the ODE solver\n",
    "module = X_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32e64e98-1723-4771-a589-9e7644bbb981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the output with respect to the input z: tensor([ 908430.5000, 1511378.5000,  735810.2500, -784080.0000, -111573.0078,\n",
      "         674891.4375])\n",
      "Gradient df/dt: tensor([ 1.1797e-07,  3.3789e-06, -5.5738e-07,  1.0000e+00,  1.0000e+00,\n",
      "         1.0000e+00], grad_fn=<AddBackward0>)\n",
      "Gradients df/dtheta: tensor([[-2.2396e+12, -1.7759e+12, -2.0931e+12, -2.1508e+11, -6.4707e+11,\n",
      "         -1.3356e+12, -1.1856e+06],\n",
      "        [-7.8870e+12, -6.2540e+12, -7.3710e+12, -7.5741e+11, -2.2787e+12,\n",
      "         -4.7036e+12, -9.4013e+05],\n",
      "        [-1.0700e+12, -8.4845e+11, -1.0000e+12, -1.0275e+11, -3.0915e+11,\n",
      "         -6.3812e+11, -1.1081e+06],\n",
      "        [ 1.1365e+10,  9.0119e+09,  1.0622e+10,  1.0914e+09,  3.2836e+09,\n",
      "          6.7779e+09, -1.1386e+05],\n",
      "        [-2.8616e+09, -2.2691e+09, -2.6744e+09, -2.7480e+08, -8.2677e+08,\n",
      "         -1.7066e+09, -3.4255e+05],\n",
      "        [-2.6955e+09, -2.1374e+09, -2.5192e+09, -2.5886e+08, -7.7879e+08,\n",
      "         -1.6075e+09, -7.0707e+05]])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the model\n",
    "f = model(z)\n",
    "\n",
    "# Create a tensor of ones, which will be the initial gradient for each component of f\n",
    "grad_output = torch.ones_like(f)\n",
    "\n",
    "# Now call backward with this grad_output\n",
    "f.backward(gradient=grad_output)\n",
    "\n",
    "###### Extracting gradients\n",
    "### df_dz\n",
    "df_dz = z.grad\n",
    "\n",
    "### df_dt\n",
    "df_dt = X_dddot(t, z, mu)\n",
    "\n",
    "### df_dtheta\n",
    "# Collect all parameter gradients\n",
    "grads = [p.grad for p in model.parameters()]\n",
    "\n",
    "# Check if the gradients exist and are in the expected form\n",
    "if grads[0] is not None and grads[1] is not None:\n",
    "    # Assuming grads[0] is the weight matrix and grads[1] is the bias vector\n",
    "    # We reshape the bias to have the same number of columns as the weight matrix and a single row\n",
    "    reshaped_bias_grad = grads[1].view(1, -1)\n",
    "    \n",
    "    # Concatenate along the rows\n",
    "    df_dtheta = torch.cat((grads[0], reshaped_bias_grad), dim=0).T\n",
    "else:\n",
    "    print(\"Gradients are not available\")\n",
    "\n",
    "print(\"Gradient of the output with respect to the input z:\", df_dz)\n",
    "print(\"Gradient df/dt:\", df_dt)\n",
    "print(\"Gradients df/dtheta:\", df_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12e92afb-d0b8-4e6c-b08f-11a254313993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.8600e+02,  6.4800e+02,  8.1000e+02,  9.7200e+02,  1.1340e+03,\n",
       "          1.2960e+03,  1.4580e+03,  1.6200e+03,  1.7820e+03,  1.9440e+03,\n",
       "          2.1060e+03,  2.2680e+03,  2.4300e+03,  2.5920e+03,  2.7540e+03,\n",
       "          2.9160e+03,  3.0780e+03,  3.2400e+03,  3.4020e+03,  3.5640e+03,\n",
       "          3.7260e+03,  3.8880e+03,  4.0500e+03,  4.2120e+03,  4.3740e+03,\n",
       "          4.5360e+03,  4.6980e+03,  4.8600e+03,  5.0220e+03,  5.1840e+03,\n",
       "          5.3460e+03,  5.5080e+03,  5.6700e+03,  5.8320e+03,  5.9940e+03,\n",
       "          6.1560e+03,  6.3180e+03,  6.4800e+03,  6.6420e+03,  6.8040e+03,\n",
       "          6.9660e+03,  7.1280e+03,  7.2900e+03,  7.4520e+03,  7.6140e+03,\n",
       "          7.7760e+03,  7.9380e+03,  8.1000e+03,  8.2620e+03,  8.4240e+03,\n",
       "          8.5860e+03,  8.7480e+03,  8.9100e+03,  9.0720e+03,  9.2340e+03,\n",
       "          9.3960e+03,  9.5580e+03,  9.7200e+03,  9.8820e+03,  1.0044e+04,\n",
       "          1.0206e+04,  1.0368e+04,  1.0530e+04,  1.0692e+04,  1.0854e+04,\n",
       "          1.1016e+04,  1.1178e+04,  1.1340e+04,  1.1502e+04,  1.1664e+04,\n",
       "          1.1826e+04,  1.1988e+04,  1.2150e+04,  1.2312e+04,  1.2474e+04,\n",
       "          1.2636e+04,  1.2798e+04,  1.2960e+04,  1.3122e+04,  1.3284e+04,\n",
       "          1.3446e+04,  1.3608e+04,  1.3770e+04,  1.3932e+04,  1.4094e+04,\n",
       "          1.4256e+04,  1.4418e+04,  1.4580e+04,  1.4742e+04,  1.4904e+04,\n",
       "          1.5066e+04,  1.5228e+04,  1.5390e+04,  1.5552e+04,  1.5714e+04,\n",
       "          1.5876e+04,  1.6038e+04,  1.6200e+04,  1.6362e+04,  1.6524e+04,\n",
       "          1.6686e+04,  1.6848e+04,  1.7010e+04,  1.7172e+04,  1.7334e+04,\n",
       "          1.7496e+04,  1.7658e+04,  1.7820e+04,  1.7982e+04,  1.8144e+04,\n",
       "          1.8306e+04,  1.8468e+04,  1.8630e+04,  1.8792e+04,  1.8954e+04,\n",
       "          1.9116e+04,  1.9278e+04,  1.9440e+04,  1.9602e+04,  1.9764e+04,\n",
       "          1.9926e+04,  2.0088e+04,  2.0250e+04,  2.0412e+04,  2.0574e+04,\n",
       "          2.0736e+04,  2.0898e+04,  2.1060e+04,  2.1222e+04,  2.1384e+04,\n",
       "          2.1546e+04,  2.1708e+04,  2.1870e+04,  2.2032e+04,  2.2194e+04,\n",
       "          2.2356e+04,  2.2518e+04,  2.2680e+04,  2.2842e+04,  2.3004e+04,\n",
       "          2.3166e+04,  2.3328e+04,  2.3490e+04,  2.3652e+04,  2.3814e+04,\n",
       "          2.3976e+04,  2.4138e+04,  2.4300e+04,  2.4462e+04,  2.4624e+04,\n",
       "          2.4786e+04,  2.4948e+04,  2.5110e+04,  2.5272e+04,  2.5434e+04,\n",
       "          2.5596e+04,  2.5758e+04,  2.5920e+04,  2.6082e+04,  2.6244e+04,\n",
       "          2.6406e+04,  2.6568e+04,  2.6730e+04,  2.6892e+04,  2.7054e+04,\n",
       "          2.7216e+04,  2.7378e+04,  2.7540e+04,  2.7702e+04,  2.7864e+04,\n",
       "          2.8026e+04,  2.8188e+04,  2.8350e+04,  2.8512e+04,  2.8674e+04,\n",
       "          2.8836e+04,  2.8998e+04,  2.9160e+04,  2.9322e+04,  2.9484e+04,\n",
       "          2.9646e+04,  2.9808e+04,  2.9970e+04,  3.0132e+04,  3.0294e+04,\n",
       "          3.0456e+04,  3.0618e+04,  3.0780e+04,  3.0942e+04,  3.1104e+04,\n",
       "          3.1266e+04,  3.1428e+04,  3.1590e+04,  3.1752e+04,  3.1914e+04,\n",
       "          3.2076e+04,  3.2238e+04,  3.2400e+04,  3.2562e+04,  3.2724e+04,\n",
       "          3.2886e+04,  3.3048e+04,  3.3210e+04,  3.3372e+04,  3.3534e+04,\n",
       "          3.3696e+04,  3.3858e+04,  3.4020e+04,  3.4182e+04,  3.4344e+04,\n",
       "          3.4506e+04,  3.4668e+04,  3.4830e+04,  3.4992e+04,  3.5154e+04,\n",
       "          3.5316e+04,  3.5478e+04,  3.5640e+04,  3.5802e+04,  3.5964e+04,\n",
       "          3.6126e+04,  3.6288e+04,  3.6450e+04,  3.6612e+04,  3.6774e+04,\n",
       "          3.6936e+04,  3.7098e+04,  3.7260e+04,  3.7422e+04,  3.7584e+04,\n",
       "          3.7746e+04,  3.7908e+04,  3.8070e+04,  3.8232e+04,  3.8394e+04,\n",
       "          3.8556e+04,  3.8718e+04,  3.8880e+04,  3.9042e+04,  3.9204e+04,\n",
       "          3.9366e+04,  3.9528e+04,  3.9690e+04,  3.9852e+04,  4.0014e+04,\n",
       "          4.0176e+04,  4.0338e+04,  4.0500e+04,  4.0662e+04,  4.0824e+04,\n",
       "          4.0986e+04,  4.1148e+04,  4.1310e+04,  4.1472e+04,  4.1634e+04,\n",
       "          4.1796e+04,  4.1958e+04,  4.2120e+04,  4.2282e+04,  4.2444e+04,\n",
       "          4.2606e+04,  4.2768e+04,  4.2930e+04,  4.3092e+04,  4.3254e+04,\n",
       "          4.3416e+04,  4.3578e+04,  4.3740e+04,  4.3902e+04,  4.4064e+04,\n",
       "          4.4226e+04,  4.4388e+04,  4.4550e+04,  4.4712e+04,  4.4874e+04,\n",
       "          4.5036e+04,  4.5198e+04,  4.5360e+04,  4.5522e+04,  4.5684e+04,\n",
       "          4.5846e+04,  4.6008e+04,  4.6170e+04,  4.6332e+04,  4.6494e+04,\n",
       "          4.6656e+04,  4.6818e+04,  4.6980e+04,  4.7142e+04,  4.7304e+04,\n",
       "          4.7466e+04,  4.7628e+04,  4.7790e+04,  4.7952e+04,  4.8114e+04,\n",
       "          4.8276e+04,  4.8438e+04,  4.8600e+04,  4.8762e+04,  4.8924e+04,\n",
       "          4.9086e+04,  4.9248e+04,  4.9410e+04,  4.9572e+04,  4.9734e+04,\n",
       "          4.9896e+04,  5.0058e+04,  5.0220e+04,  5.0382e+04,  5.0544e+04,\n",
       "          5.0706e+04,  5.0868e+04,  5.1030e+04,  5.1192e+04,  5.1354e+04,\n",
       "          5.1516e+04,  5.1678e+04,  5.1840e+04,  5.2002e+04,  5.2164e+04,\n",
       "          5.2326e+04,  5.2488e+04,  5.2650e+04,  5.2812e+04,  5.2974e+04,\n",
       "          5.3136e+04,  5.3298e+04,  5.3460e+04,  5.3622e+04,  5.3784e+04,\n",
       "          5.3946e+04,  5.4108e+04,  5.4270e+04,  5.4432e+04,  5.4594e+04,\n",
       "          5.4756e+04,  5.4918e+04,  5.5080e+04,  5.5242e+04,  5.5404e+04,\n",
       "          5.5566e+04,  5.5728e+04,  5.5890e+04,  5.6052e+04,  5.6214e+04,\n",
       "          5.6376e+04,  5.6538e+04,  5.6700e+04,  5.6862e+04,  5.7024e+04,\n",
       "          5.7186e+04,  5.7348e+04,  5.7510e+04,  5.7672e+04,  5.7834e+04,\n",
       "          5.7996e+04,  5.8158e+04,  5.8320e+04,  5.8482e+04,  5.8644e+04,\n",
       "          5.8806e+04,  5.8968e+04,  5.9130e+04,  5.9292e+04,  5.9454e+04,\n",
       "          5.9616e+04,  5.9778e+04,  5.9940e+04,  6.0102e+04,  6.0264e+04,\n",
       "          6.0426e+04,  6.0588e+04,  6.0750e+04,  6.0912e+04,  6.1074e+04,\n",
       "          6.1236e+04,  6.1398e+04,  6.1560e+04,  6.1722e+04,  6.1884e+04,\n",
       "          6.2046e+04,  6.2208e+04,  6.2370e+04,  6.2532e+04,  6.2694e+04,\n",
       "          6.2856e+04,  6.3018e+04,  6.3180e+04,  6.3342e+04,  6.3504e+04,\n",
       "          6.3666e+04,  6.3828e+04,  6.3990e+04,  6.4152e+04,  6.4314e+04,\n",
       "          6.4476e+04,  6.4638e+04, -0.0000e+00, -1.5501e-01, -3.1002e-01],\n",
       "        requires_grad=True),\n",
       " tensor(-4.7336e+12),\n",
       " tensor([1.4753e+19, 1.1698e+19, 1.3787e+19, 1.4167e+18, 4.2623e+18, 8.7980e+18,\n",
       "         3.6630e+12]),\n",
       " tensor(220756.8125, grad_fn=<DotBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_dynamics_new(z, t, theta, a, module, df_dz, df_dt, df_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b692b-c844-4454-bf41-d0379bc3c673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
